{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 13975686,
     "sourceType": "datasetVersion",
     "datasetId": 8909948
    },
    {
     "sourceId": 13996222,
     "sourceType": "datasetVersion",
     "datasetId": 8919782
    },
    {
     "sourceId": 14170895,
     "sourceType": "datasetVersion",
     "datasetId": 9032718
    },
    {
     "sourceId": 14178789,
     "sourceType": "datasetVersion",
     "datasetId": 9038625
    }
   ],
   "dockerImageVersionId": 31193,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-15T13:43:56.864527Z",
     "iopub.execute_input": "2025-12-15T13:43:56.864682Z",
     "iopub.status.idle": "2025-12-15T13:43:56.877821Z",
     "shell.execute_reply.started": "2025-12-15T13:43:56.864670Z",
     "shell.execute_reply": "2025-12-15T13:43:56.877307Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "mind_data_path = \"/kaggle/input/mind-data/mind_data\"  \nos.makedirs(mind_data_path, exist_ok=True) ",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-15T13:43:56.879403Z",
     "iopub.execute_input": "2025-12-15T13:43:56.879578Z",
     "iopub.status.idle": "2025-12-15T13:43:56.908557Z",
     "shell.execute_reply.started": "2025-12-15T13:43:56.879564Z",
     "shell.execute_reply": "2025-12-15T13:43:56.907901Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install tensorflow==2.15.0 \"numpy<2.0\"",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-15T13:43:56.909287Z",
     "iopub.execute_input": "2025-12-15T13:43:56.910056Z",
     "iopub.status.idle": "2025-12-15T13:44:28.659060Z",
     "shell.execute_reply.started": "2025-12-15T13:43:56.910035Z",
     "shell.execute_reply": "2025-12-15T13:44:28.658177Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# CÃ i cuDNN 8.9 tá»« pip\n!pip uninstall tensorflow -y\n!pip install nvidia-cudnn-cu12==8.9.7.29\n!pip install tensorflow==2.15.0\n\n# Thiáº¿t láº­p LD_LIBRARY_PATH\nimport os\ncudnn_path = '/usr/local/lib/python3.11/dist-packages/nvidia/cudnn/lib'\nos.environ['LD_LIBRARY_PATH'] = f\"{cudnn_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n\nprint(\"âœ… cuDNN 8.9 installed. Please RESTART KERNEL!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\ncudnn_path = '/usr/local/lib/python3.11/dist-packages/nvidia/cudnn/lib'\nos.environ['LD_LIBRARY_PATH'] = f\"{cudnn_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n\nimport tensorflow as tf\nprint(\"GPUs:\", tf.config.list_physical_devices('GPU'))",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 1: Monkey patch Ä‘á»ƒ force embedding lÃªn GPU\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\nimport tensorflow as tf\n\n# Monkey patch Embedding layer Ä‘á»ƒ force GPU placement\noriginal_embedding_init = tf.keras.layers.Embedding.__init__\n\ndef patched_embedding_init(self, *args, **kwargs):\n    # Force embeddings on GPU\n    with tf.device('/GPU:0'):\n        original_embedding_init(self, *args, **kwargs)\n\ntf.keras.layers.Embedding.__init__ = patched_embedding_init\n\nprint(\"âœ… Patched Embedding layer to use GPU\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Setup\nfrom recommenders.models.newsrec.newsrec_utils import prepare_hparams  \nfrom recommenders.models.newsrec.models.nrms import NRMSModel  \nfrom recommenders.models.newsrec.io.mind_iterator import MINDIterator  \n\ntrain_news_file = os.path.join(mind_data_path, \"train\", \"news.tsv\")  \ntrain_behaviors_file = os.path.join(mind_data_path, \"train\", \"behaviors.tsv\")  \nvalid_news_file = os.path.join(mind_data_path, \"valid\", \"news.tsv\")  \nvalid_behaviors_file = os.path.join(mind_data_path, \"valid\", \"behaviors.tsv\")  \nwordEmb_file = os.path.join(mind_data_path, \"utils\", \"embedding.npy\")  \nuserDict_file = os.path.join(mind_data_path, \"utils\", \"uid2index.pkl\")  \nwordDict_file = os.path.join(mind_data_path, \"utils\", \"word_dict.pkl\")  \nyaml_file = os.path.join(mind_data_path, \"utils\", \"nrms.yaml\")  \n\nhparams = prepare_hparams(  \n    yaml_file,  \n    wordEmb_file=wordEmb_file,  \n    wordDict_file=wordDict_file,  \n    userDict_file=userDict_file,  \n    epochs=3,\n    batch_size=384\n)  \n\n# Create model with GPU\nwith tf.device('/GPU:0'):\n    iterator = MINDIterator  \n    model = NRMSModel(hparams, iterator, seed=42)\n\nprint(\"âœ… Model created on GPU\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Load checkpoint\ncheckpoint_path = \"/kaggle/input/model-epoch-6-10/model/nrms_ckpt\"\nmodel.model.load_weights(checkpoint_path)\nprint(f\"âœ… Checkpoint loaded\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Training\nprint(\"ðŸ”¥ Starting training...\")\nmodel.fit(  \n    train_news_file,\n    train_behaviors_file,\n    valid_news_file,\n    valid_behaviors_file\n)\nprint(\"âœ… Training completed!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ÄÃ¡nh giÃ¡ trÃªn validation set  \neval_results = model.run_eval(valid_news_file, valid_behaviors_file)  \nprint(\"Káº¿t quáº£ Ä‘Ã¡nh giÃ¡:\")  \nfor metric, value in eval_results.items():  \n    print(f\"{metric}: {value:.4f}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model_path = os.path.join(\"/kaggle/working/\", \"model\")\nos.makedirs(model_path, exist_ok=True)\n\nmodel.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from tqdm import tqdm\nimport numpy as np\n\ntest_behaviors_file = \"/kaggle/input/mind-data/mind_data/test/behaviors.tsv\"  \ntest_news_file = \"/kaggle/input/mind-data/mind_data/test/news.tsv\"\n\nprint(\"ðŸ”§ Patching iterator for test set (no labels)...\")\n\n# Backup original method\noriginal_init = model.test_iterator.init_behaviors\n\ndef init_behaviors_no_labels(behaviors_file):\n    \"\"\"Modified init_behaviors for test set without labels\"\"\"\n    model.test_iterator.histories = []\n    model.test_iterator.imprs = []\n    model.test_iterator.labels = []\n    model.test_iterator.impr_indexes = []\n    model.test_iterator.uindexes = []\n\n    with open(behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n        impr_index = 0\n        for line in rd:\n            uid, time, history, impr = line.strip(\"\\n\").split(\"\\t\")[-4:]\n\n            # Parse history\n            history = [model.test_iterator.nid2index[i] for i in history.split() if i in model.test_iterator.nid2index]\n            history = [0] * (model.test_iterator.his_size - len(history)) + history[:model.test_iterator.his_size]\n\n            # Parse impressions - TEST SET KHÃ”NG CÃ“ LABEL\n            impr_news = []\n            for item in impr.split():\n                # Test set: chá»‰ cÃ³ news_id, KHÃ”NG cÃ³ \"-0\" hay \"-1\"\n                if \"-\" in item:\n                    # Validation/train set format: N12345-1\n                    news_id = item.split(\"-\")[0]\n                else:\n                    # Test set format: N12345\n                    news_id = item\n                \n                if news_id in model.test_iterator.nid2index:\n                    impr_news.append(model.test_iterator.nid2index[news_id])\n            \n            # Táº¡o dummy labels (khÃ´ng dÃ¹ng cho test)\n            label = [0] * len(impr_news)\n            \n            uindex = model.test_iterator.uid2index[uid] if uid in model.test_iterator.uid2index else 0\n\n            model.test_iterator.histories.append(history)\n            model.test_iterator.imprs.append(impr_news)\n            model.test_iterator.labels.append(label)\n            model.test_iterator.impr_indexes.append(impr_index)\n            model.test_iterator.uindexes.append(uindex)\n            impr_index += 1\n\n# Apply patch\nmodel.test_iterator.init_behaviors = init_behaviors_no_labels\n\nprint(\"âœ… Iterator patched for test set\")\n\n# Run evaluationa\nprint(\"\\nðŸ” Running evaluation on test set...\")\nprint(\"   Model will use trained weights to generate rankings\")\n\ngroup_impr_indexes, group_labels, group_preds = model.run_fast_eval(\n    test_news_file, test_behaviors_file\n)\n\nprint(f\"\\nâœ… Generated predictions for {len(group_impr_indexes)} impressions\")\n\n# Write predictions\nprint(\"\\nðŸ’¾ Writing predictions to file...\")\nprediction_file = \"/kaggle/working/prediction.txt\"\n\nwith open(prediction_file, 'w') as f:  \n    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds), \n                                   total=len(group_impr_indexes),\n                                   desc=\"Writing\"):  \n        # MIND competition format: impression_id báº¯t Ä‘áº§u tá»« 1\n        impr_id = impr_index + 1\n        \n        # Calculate rankings (score cao nháº¥t = rank 1)\n        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()  \n        pred_rank_str = '[' + ','.join([str(i) for i in pred_rank]) + ']'  \n        \n        f.write(f\"{impr_id} {pred_rank_str}\\n\")\n\nprint(f\"\\nâœ… Prediction file saved to: {prediction_file}\")\n\n# Validate output\nprint(\"\\nðŸ” Validating output format...\")\nwith open(prediction_file, 'r') as f:\n    lines = f.readlines()\n    print(f\"   Total predictions: {len(lines)}\")\n    print(f\"   First 3 lines:\")\n    for i in range(min(3, len(lines))):\n        parts = lines[i].strip().split()\n        print(f\"      ImprID={parts[0]}, Rankings={parts[1][:50]}{'...' if len(parts[1]) > 50 else ''}\")\n\nprint(\"\\nâœ… File ready to submit to MIND competition!\")\nprint(f\"   Download: {prediction_file}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-05T04:34:58.590887Z",
     "iopub.execute_input": "2025-12-05T04:34:58.591562Z",
     "iopub.status.idle": "2025-12-05T05:05:48.960787Z",
     "shell.execute_reply.started": "2025-12-05T04:34:58.591533Z",
     "shell.execute_reply": "2025-12-05T05:05:48.960128Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
