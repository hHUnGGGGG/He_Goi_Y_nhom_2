{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 13975686,
     "sourceType": "datasetVersion",
     "datasetId": 8909948
    },
    {
     "sourceId": 13996222,
     "sourceType": "datasetVersion",
     "datasetId": 8919782
    },
    {
     "sourceId": 14170895,
     "sourceType": "datasetVersion",
     "datasetId": 9032718
    }
   ],
   "dockerImageVersionId": 31193,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import os",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:23:38.478067Z",
     "iopub.execute_input": "2025-12-22T14:23:38.478886Z",
     "iopub.status.idle": "2025-12-22T14:23:38.482391Z",
     "shell.execute_reply.started": "2025-12-22T14:23:38.478855Z",
     "shell.execute_reply": "2025-12-22T14:23:38.481646Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "mind_data_path = \"/kaggle/input/mind-data/mind_data\"  \nos.makedirs(mind_data_path, exist_ok=True) ",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:23:40.998313Z",
     "iopub.execute_input": "2025-12-22T14:23:40.999044Z",
     "iopub.status.idle": "2025-12-22T14:23:41.00466Z",
     "shell.execute_reply.started": "2025-12-22T14:23:40.999018Z",
     "shell.execute_reply": "2025-12-22T14:23:41.003984Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install tensorflow==2.15.0 \"numpy<2.0\"",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:23:43.89233Z",
     "iopub.execute_input": "2025-12-22T14:23:43.893047Z",
     "iopub.status.idle": "2025-12-22T14:24:15.874994Z",
     "shell.execute_reply.started": "2025-12-22T14:23:43.893024Z",
     "shell.execute_reply": "2025-12-22T14:24:15.873755Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# CÃ i cuDNN 8.9 tá»« pip\n!pip uninstall tensorflow -y\n!pip install nvidia-cudnn-cu12==8.9.7.29\n!pip install tensorflow==2.15.0\n\n# Thiáº¿t láº­p LD_LIBRARY_PATH\nimport os\ncudnn_path = '/usr/local/lib/python3.11/dist-packages/nvidia/cudnn/lib'\nos.environ['LD_LIBRARY_PATH'] = f\"{cudnn_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n\nprint(\"âœ… cuDNN 8.9 installed. Please RESTART KERNEL!\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:24:55.320241Z",
     "iopub.execute_input": "2025-12-22T14:24:55.320941Z",
     "iopub.status.idle": "2025-12-22T14:25:45.821014Z",
     "shell.execute_reply.started": "2025-12-22T14:24:55.320909Z",
     "shell.execute_reply": "2025-12-22T14:25:45.820241Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\ncudnn_path = '/usr/local/lib/python3.11/dist-packages/nvidia/cudnn/lib'\nos.environ['LD_LIBRARY_PATH'] = f\"{cudnn_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n\nimport tensorflow as tf\nprint(\"GPUs:\", tf.config.list_physical_devices('GPU'))",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:28:29.482627Z",
     "iopub.execute_input": "2025-12-22T14:28:29.482981Z",
     "iopub.status.idle": "2025-12-22T14:28:38.234019Z",
     "shell.execute_reply.started": "2025-12-22T14:28:29.482953Z",
     "shell.execute_reply": "2025-12-22T14:28:38.233324Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\nos.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding as KEmbedding  # láº¥y class gá»‘c\n\n# Náº¿u Ä‘Ã£ patch trÆ°á»›c Ä‘Ã³ (trong cÃ¹ng session), restore vá» hÃ m gá»‘c Keras\ntf.keras.layers.Embedding.__init__ = KEmbedding.__init__\n\n\ntf.keras.layers.Embedding._true_init = KEmbedding.__init__\n\ndef patched_embedding_init(self, *args, **kwargs):\n    with tf.device(\"/GPU:0\"):\n        tf.keras.layers.Embedding._true_init(self, *args, **kwargs)\n\ntf.keras.layers.Embedding.__init__ = patched_embedding_init\nprint(\"âœ… Patched Embedding (safe v2, no recursion)\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:28:40.70856Z",
     "iopub.execute_input": "2025-12-22T14:28:40.709279Z",
     "iopub.status.idle": "2025-12-22T14:28:40.733949Z",
     "shell.execute_reply.started": "2025-12-22T14:28:40.7092Z",
     "shell.execute_reply": "2025-12-22T14:28:40.733349Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "lstur_yaml = \"\"\"\ndata:\n  title_size: 30\n  his_size: 50\n  body_size: 50\n  cat_num: 17\n  subcat_num: 249\n  data_format: news\n  npratio: 4\n\nmodel:\n  # ===== required by your recommenders version =====\n  filter_num: 300\n  window_size: 3\n  cnn_activation: relu\n\n  # ===== shared / existing =====\n  attention_hidden_dim: 300\n  word_emb_dim: 300\n  cat_emb_dim: 100\n  subcat_emb_dim: 100\n  dropout: 0.2\n\n  # ===== keep these (some versions still expect them) =====\n  head_num: 20\n  head_dim: 20\n\n  # ===== switch to LSTUR =====\n  model_type: lstur\n  gru_unit: 300\n  user_emb_dim: 50\n  type: con   # ini or con\n\ntraining:\n  batch_size: 128\n  epochs: 10\n  learning_rate: 0.0005\n  loss: cross_entropy_loss\n  optimizer: adam\n  support_quick_scoring: true\n\ninfo:\n  metrics: [\"group_auc\", \"mean_mrr\", \"ndcg@5;10\"]\n  show_step: 100000\n\nfiles:\n  wordEmb_file: \"utils/embedding.npy\"\n  wordDict_file: \"utils/word_dict.pkl\"\n  userDict_file: \"utils/uid2index.pkl\"\n\"\"\"\n\nwith open(\"/kaggle/working/lstur.yaml\", \"w\") as f:\n    f.write(lstur_yaml)\n\nprint(\"âœ… wrote /kaggle/working/lstur.yaml\")\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:28:49.762185Z",
     "iopub.execute_input": "2025-12-22T14:28:49.76246Z",
     "iopub.status.idle": "2025-12-22T14:28:49.768112Z",
     "shell.execute_reply.started": "2025-12-22T14:28:49.762441Z",
     "shell.execute_reply": "2025-12-22T14:28:49.767409Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Setup\nfrom recommenders.models.newsrec.newsrec_utils import prepare_hparams  \nfrom recommenders.models.newsrec.models.lstur import LSTURModel  \nfrom recommenders.models.newsrec.io.mind_iterator import MINDIterator  \n\ntrain_news_file = os.path.join(mind_data_path, \"train\", \"news.tsv\")  \ntrain_behaviors_file = os.path.join(mind_data_path, \"train\", \"behaviors.tsv\")  \nvalid_news_file = os.path.join(mind_data_path, \"valid\", \"news.tsv\")  \nvalid_behaviors_file = os.path.join(mind_data_path, \"valid\", \"behaviors.tsv\")  \nwordEmb_file = os.path.join(mind_data_path, \"utils\", \"embedding.npy\")  \nuserDict_file = os.path.join(mind_data_path, \"utils\", \"uid2index.pkl\")  \nwordDict_file = os.path.join(mind_data_path, \"utils\", \"word_dict.pkl\")  \nyaml_file = \"/kaggle/working/lstur.yaml\"\nprint(\"Using yaml:\", yaml_file)\n\nhparams = prepare_hparams(  \n    yaml_file,  \n    wordEmb_file=wordEmb_file,  \n    wordDict_file=wordDict_file,  \n    userDict_file=userDict_file,  \n    epochs=20,\n    batch_size=128\n)  \nhparams.metrics = [\"mean_mrr\", \"ndcg@5\", \"ndcg@10\"]\nhparams.early_stop_metric = \"ndcg@5\"\nhparams.early_stop_patience = 3\nhparams.show_step = 50\nhparams.learning_rate = 1e-4\n# Create model with GPU\nwith tf.device('/GPU:0'):\n    iterator = MINDIterator  \n    model = LSTURModel(hparams, iterator, seed=42)\n\nprint(\"âœ… LSTURModel created on GPU\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:28:54.70836Z",
     "iopub.execute_input": "2025-12-22T14:28:54.709117Z",
     "iopub.status.idle": "2025-12-22T14:28:59.746964Z",
     "shell.execute_reply.started": "2025-12-22T14:28:54.70909Z",
     "shell.execute_reply": "2025-12-22T14:28:59.746222Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os, re, glob\n\nCKPT_DIR = \"/kaggle/working/checkpoints_lstur\"\nos.makedirs(CKPT_DIR, exist_ok=True)\n\ndef ckpt_path(epoch: int) -> str:\n    # Keras save_weights sáº½ táº¡o ra file theo prefix, nÃªn ta dÃ¹ng prefix rÃµ rÃ ng\n    return os.path.join(CKPT_DIR, f\"lstur_epoch_{epoch:03d}\")\n\ndef save_ckpt(epoch: int):\n    path = ckpt_path(epoch)\n    model.model.save_weights(path)\n    print(f\"âœ… Saved checkpoint: {path}\")\n\ndef list_ckpts():\n    # save_weights táº¡o ra nhiá»u file (vd .index/.data) hoáº·c 1 prefix tuá»³ backend\n    # ta list theo prefix\n    prefixes = sorted(set(\n        p.replace(\".index\",\"\").replace(\".data-00000-of-00001\",\"\")\n        for p in glob.glob(os.path.join(CKPT_DIR, \"lstur_epoch_*\"))\n    ))\n    return prefixes\n\ndef latest_ckpt():\n    prefixes = list_ckpts()\n    if not prefixes:\n        return None, 0\n    # láº¥y epoch lá»›n nháº¥t tá»« tÃªn\n    last = prefixes[-1]\n    m = re.search(r\"lstur_epoch_(\\d+)$\", last)\n    ep = int(m.group(1)) if m else 0\n    return last, ep\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:29:32.558374Z",
     "iopub.execute_input": "2025-12-22T14:29:32.558676Z",
     "iopub.status.idle": "2025-12-22T14:29:32.565649Z",
     "shell.execute_reply.started": "2025-12-22T14:29:32.558653Z",
     "shell.execute_reply": "2025-12-22T14:29:32.56497Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#Training\nprint(\"ðŸ”¥ Starting training...\")\nhparams.epochs = 1\nNUM_EPOCHS = 10 \n\nfor ep in range(1, NUM_EPOCHS + 1):\n    print(f\"\\n===== EPOCH {ep}/{NUM_EPOCHS} =====\")\n    \n    model.fit(\n        train_news_file,\n        train_behaviors_file,\n        valid_news_file,\n        valid_behaviors_file,\n    )\n    \n    # (tuá»³ chá»n) eval Ä‘á»ƒ theo dÃµi metric\n    eval_results = model.run_eval(valid_news_file, valid_behaviors_file)\n    print(\"Eval:\", eval_results)\n    \n    # lÆ°u checkpoint epoch nÃ y\n    save_ckpt(ep)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-22T14:29:57.54234Z",
     "iopub.execute_input": "2025-12-22T14:29:57.542968Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model = LSTURModel(hparams, MINDIterator, seed=42)\n\nckpt, last_ep = latest_ckpt()\nprint(\"Latest:\", ckpt, \"epoch:\", last_ep)\n\nif ckpt is not None:\n    model.model.load_weights(ckpt)\n    print(f\"âœ… Loaded checkpoint epoch {last_ep}\")\nelse:\n    print(\"âš ï¸ No checkpoint found, training from scratch.\")\n",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ÄÃ¡nh giÃ¡ trÃªn validation set  \neval_results = model.run_eval(valid_news_file, valid_behaviors_file)  \nprint(\"Káº¿t quáº£ Ä‘Ã¡nh giÃ¡:\")  \nfor metric, value in eval_results.items():  \n    print(f\"{metric}: {value:.4f}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model_path = os.path.join(\"/kaggle/working/\", \"model\")\nos.makedirs(model_path, exist_ok=True)\n\nmodel.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from tqdm import tqdm\nimport numpy as np\n\ntest_behaviors_file = \"/kaggle/input/mind-data/mind_data/test/behaviors.tsv\"  \ntest_news_file = \"/kaggle/input/mind-data/mind_data/test/news.tsv\"\n\nprint(\"ðŸ”§ Patching iterator for test set (no labels)...\")\n\n# Backup original method\noriginal_init = model.test_iterator.init_behaviors\n\ndef init_behaviors_no_labels(behaviors_file):\n    \"\"\"Modified init_behaviors for test set without labels\"\"\"\n    model.test_iterator.histories = []\n    model.test_iterator.imprs = []\n    model.test_iterator.labels = []\n    model.test_iterator.impr_indexes = []\n    model.test_iterator.uindexes = []\n\n    with open(behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n        impr_index = 0\n        for line in rd:\n            uid, time, history, impr = line.strip(\"\\n\").split(\"\\t\")[-4:]\n\n            # Parse history\n            history = [model.test_iterator.nid2index[i] for i in history.split() if i in model.test_iterator.nid2index]\n            history = [0] * (model.test_iterator.his_size - len(history)) + history[:model.test_iterator.his_size]\n\n            # Parse impressions - TEST SET KHÃ”NG CÃ“ LABEL\n            impr_news = []\n            for item in impr.split():\n                # Test set: chá»‰ cÃ³ news_id, KHÃ”NG cÃ³ \"-0\" hay \"-1\"\n                if \"-\" in item:\n                    # Validation/train set format: N12345-1\n                    news_id = item.split(\"-\")[0]\n                else:\n                    # Test set format: N12345\n                    news_id = item\n                \n                if news_id in model.test_iterator.nid2index:\n                    impr_news.append(model.test_iterator.nid2index[news_id])\n            \n            # Táº¡o dummy labels (khÃ´ng dÃ¹ng cho test)\n            label = [0] * len(impr_news)\n            \n            uindex = model.test_iterator.uid2index[uid] if uid in model.test_iterator.uid2index else 0\n\n            model.test_iterator.histories.append(history)\n            model.test_iterator.imprs.append(impr_news)\n            model.test_iterator.labels.append(label)\n            model.test_iterator.impr_indexes.append(impr_index)\n            model.test_iterator.uindexes.append(uindex)\n            impr_index += 1\n\n# Apply patch\nmodel.test_iterator.init_behaviors = init_behaviors_no_labels\n\nprint(\"âœ… Iterator patched for test set\")\n\n# Run evaluationa\nprint(\"\\nðŸ” Running evaluation on test set...\")\nprint(\"   Model will use trained weights to generate rankings\")\n\ngroup_impr_indexes, group_labels, group_preds = model.run_fast_eval(\n    test_news_file, test_behaviors_file\n)\n\nprint(f\"\\nâœ… Generated predictions for {len(group_impr_indexes)} impressions\")\n\n# Write predictions\nprint(\"\\nðŸ’¾ Writing predictions to file...\")\nprediction_file = \"/kaggle/working/prediction.txt\"\n\nwith open(prediction_file, 'w') as f:  \n    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds), \n                                   total=len(group_impr_indexes),\n                                   desc=\"Writing\"):  \n        # MIND competition format: impression_id báº¯t Ä‘áº§u tá»« 1\n        impr_id = impr_index + 1\n        \n        # Calculate rankings (score cao nháº¥t = rank 1)\n        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()  \n        pred_rank_str = '[' + ','.join([str(i) for i in pred_rank]) + ']'  \n        \n        f.write(f\"{impr_id} {pred_rank_str}\\n\")\n\nprint(f\"\\nâœ… Prediction file saved to: {prediction_file}\")\n\n# Validate output\nprint(\"\\nðŸ” Validating output format...\")\nwith open(prediction_file, 'r') as f:\n    lines = f.readlines()\n    print(f\"   Total predictions: {len(lines)}\")\n    print(f\"   First 3 lines:\")\n    for i in range(min(3, len(lines))):\n        parts = lines[i].strip().split()\n        print(f\"      ImprID={parts[0]}, Rankings={parts[1][:50]}{'...' if len(parts[1]) > 50 else ''}\")\n\nprint(\"\\nâœ… File ready to submit to MIND competition!\")\nprint(f\"   Download: {prediction_file}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-12-05T04:34:58.590887Z",
     "iopub.execute_input": "2025-12-05T04:34:58.591562Z",
     "iopub.status.idle": "2025-12-05T05:05:48.960787Z",
     "shell.execute_reply.started": "2025-12-05T04:34:58.591533Z",
     "shell.execute_reply": "2025-12-05T05:05:48.960128Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
