{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 14240634,
     "sourceType": "datasetVersion",
     "datasetId": 9047395
    }
   ],
   "dockerImageVersionId": 31193,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "!python --version",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "mind_data_path = \"/kaggle/input/mind-data/mind_data\"  \nos.makedirs(mind_data_path, exist_ok=True) ",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "!pip install tensorflow==2.15.0 \"numpy<2.0\"",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# C√†i cuDNN 8.9 t·ª´ pip\n!pip uninstall tensorflow -y\n!pip install nvidia-cudnn-cu12==8.9.7.29\n!pip install tensorflow==2.15.0\n\n# Thi·∫øt l·∫≠p LD_LIBRARY_PATH\nimport os\ncudnn_path = '/usr/local/lib/python3.11/dist-packages/nvidia/cudnn/lib'\nos.environ['LD_LIBRARY_PATH'] = f\"{cudnn_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n\nprint(\"‚úÖ cuDNN 8.9 installed. Please RESTART KERNEL!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import os\ncudnn_path = '/usr/local/lib/python3.11/dist-packages/nvidia/cudnn/lib'\nos.environ['LD_LIBRARY_PATH'] = f\"{cudnn_path}:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n\nimport tensorflow as tf\nprint(\"GPUs:\", tf.config.list_physical_devices('GPU'))",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 1: Monkey patch ƒë·ªÉ force embedding l√™n GPU\nimport os\nos.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n\nimport tensorflow as tf\n\n# Monkey patch Embedding layer ƒë·ªÉ force GPU placement\noriginal_embedding_init = tf.keras.layers.Embedding.__init__\n\ndef patched_embedding_init(self, *args, **kwargs):\n    # Force embeddings on GPU\n    with tf.device('/GPU:0'):\n        original_embedding_init(self, *args, **kwargs)\n\ntf.keras.layers.Embedding.__init__ = patched_embedding_init\n\nprint(\"‚úÖ Patched Embedding layer to use GPU\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Setup\nfrom recommenders.models.newsrec.newsrec_utils import prepare_hparams  \nfrom recommenders.models.newsrec.models.npa import NPAModel\nfrom recommenders.models.newsrec.io.mind_iterator import MINDIterator  \n\ntrain_news_file = os.path.join(mind_data_path, \"train\", \"news.tsv\")  \ntrain_behaviors_file = os.path.join(mind_data_path, \"train\", \"behaviors.tsv\")  \nvalid_news_file = os.path.join(mind_data_path, \"valid\", \"news.tsv\")  \nvalid_behaviors_file = os.path.join(mind_data_path, \"valid\", \"behaviors.tsv\")  \nwordEmb_file = os.path.join(mind_data_path, \"utils\", \"embedding.npy\")  \nuserDict_file = os.path.join(mind_data_path, \"utils\", \"uid2index.pkl\")  \nwordDict_file = os.path.join(mind_data_path, \"utils\", \"word_dict.pkl\")  \nyaml_file = os.path.join(mind_data_path, \"utils\", \"npa.yaml\")  \n\nhparams = prepare_hparams(  \n    yaml_file,  \n    wordEmb_file=wordEmb_file,  \n    wordDict_file=wordDict_file,  \n    userDict_file=userDict_file,  \n    epochs=1,\n    batch_size=384\n)\n\n# Create model with GPU\nwith tf.device('/GPU:0'):\n    iterator = MINDIterator  \n    model = NPAModel(hparams, iterator, seed=42)\n\nprint(\"‚úÖ Model created on GPU\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def disable_quick_scoring(m):\n    if hasattr(m, \"support_quick_scoring\"):\n        m.support_quick_scoring = False\n    if hasattr(m, \"hparams\") and hasattr(m.hparams, \"support_quick_scoring\"):\n        m.hparams.support_quick_scoring = False\n\ndisable_quick_scoring(model)\n\n# Build encoders required by fast eval\ndef _get_submodules(m):\n    try:\n        return list(getattr(m, \"submodules\", []))\n    except Exception:\n        return []\n\n_submodules = _get_submodules(model.model)\n_embedding_layers = [m for m in _submodules if isinstance(m, tf.keras.layers.Embedding)]\n\nembedding_layer = _embedding_layers[0] if len(_embedding_layers) >= 1 else None\nuser_embedding_layer = _embedding_layers[1] if len(_embedding_layers) >= 2 else embedding_layer\n\nif not hasattr(model, \"newsencoder\") and hasattr(model, \"_build_newsencoder\"):\n    model.newsencoder = model._build_newsencoder(embedding_layer, user_embedding_layer)\n\n_submodules = _get_submodules(model.model)\n_titleencoder_candidates = []\nfor m in _submodules:\n    try:\n        if isinstance(m, tf.keras.Model):\n            n = getattr(m, \"name\", \"\")\n            if \"title\" in n.lower():\n                _titleencoder_candidates.append(m)\n    except Exception:\n        pass\n\nif not _titleencoder_candidates:\n    for m in _submodules:\n        try:\n            if isinstance(m, tf.keras.Model):\n                n = getattr(m, \"name\", \"\")\n                if \"encoder\" in n.lower() and \"user\" not in n.lower():\n                    _titleencoder_candidates.append(m)\n        except Exception:\n            pass\n\ntitleencoder = _titleencoder_candidates[0] if _titleencoder_candidates else None\n\nif not hasattr(model, \"userencoder\") and hasattr(model, \"_build_userencoder\"):\n    model.userencoder = model._build_userencoder(titleencoder, user_embedding_layer)\n\nprint(\"‚úÖ Build encoders success\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Check model dir\nprint(\"Wrapper encoder attrs:\", [x for x in dir(model) if \"encod\" in x.lower()])\nprint(\"Inner model type:\", type(model.model))\nprint(\"Inner model encoder attrs:\", [x for x in dir(model.model) if \"encod\" in x.lower()])\n\n# Smoke test (fail-fast): eval -> short train -> eval\nprint(\"üß™ Smoke test: checking encoders...\")\nprint(\"has newsencoder:\", hasattr(model, \"newsencoder\"))\nprint(\"has userencoder:\", hasattr(model, \"userencoder\"))\nassert hasattr(model, \"newsencoder\"), \"Missing model.newsencoder (needed for fast eval)\"\nassert hasattr(model, \"userencoder\"), \"Missing model.userencoder (needed for fast eval)\"\n\nprint(\"‚úÖ Test passed\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Load checkpoint\n# checkpoint_path = \"/kaggle/input/model-epoch-4-6/model/npa_ckpt\"\n# model.model.load_weights(checkpoint_path)\n# print(f\"‚úÖ Checkpoint loaded\")\n\nprint(f\"‚úÖ First time\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Training\nprint(\"üî• Starting training...\")\nmodel.fit(  \n    train_news_file,\n    train_behaviors_file,\n    valid_news_file,\n    valid_behaviors_file\n)\nprint(\"‚úÖ Training completed!\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# ƒê√°nh gi√° tr√™n validation set  \neval_results = model.run_eval(valid_news_file, valid_behaviors_file)  \nprint(\"K·∫øt qu·∫£ ƒë√°nh gi√°:\")  \nfor metric, value in eval_results.items():  \n    print(f\"{metric}: {value:.4f}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model_path = os.path.join(\"/kaggle/working/\", \"model\")\nos.makedirs(model_path, exist_ok=True)\n\nmodel.model.save_weights(os.path.join(model_path, \"npa_ckpt\"))",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "from tqdm import tqdm\nimport numpy as np\n\ntest_behaviors_file = \"/kaggle/input/mind-data/mind_data/test/behaviors.tsv\"  \ntest_news_file = \"/kaggle/input/mind-data/mind_data/test/news.tsv\"\n\nprint(\"üîß Patching iterator for test set (no labels)...\")\n\n# Backup original method\noriginal_init = model.test_iterator.init_behaviors\n\ndef init_behaviors_no_labels(behaviors_file):\n    \"\"\"Modified init_behaviors for test set without labels\"\"\"\n    model.test_iterator.histories = []\n    model.test_iterator.imprs = []\n    model.test_iterator.labels = []\n    model.test_iterator.impr_indexes = []\n    model.test_iterator.uindexes = []\n\n    with open(behaviors_file, \"r\", encoding=\"utf-8\") as rd:\n        impr_index = 0\n        for line in rd:\n            uid, time, history, impr = line.strip(\"\\n\").split(\"\\t\")[-4:]\n\n            # Parse history\n            history = [model.test_iterator.nid2index[i] for i in history.split() if i in model.test_iterator.nid2index]\n            history = [0] * (model.test_iterator.his_size - len(history)) + history[:model.test_iterator.his_size]\n\n            # Parse impressions - TEST SET KH√îNG C√ì LABEL\n            impr_news = []\n            for item in impr.split():\n                # Test set: ch·ªâ c√≥ news_id, KH√îNG c√≥ \"-0\" hay \"-1\"\n                if \"-\" in item:\n                    # Validation/train set format: N12345-1\n                    news_id = item.split(\"-\")[0]\n                else:\n                    # Test set format: N12345\n                    news_id = item\n                \n                if news_id in model.test_iterator.nid2index:\n                    impr_news.append(model.test_iterator.nid2index[news_id])\n            \n            # T·∫°o dummy labels (kh√¥ng d√πng cho test)\n            label = [0] * len(impr_news)\n            \n            uindex = model.test_iterator.uid2index[uid] if uid in model.test_iterator.uid2index else 0\n\n            model.test_iterator.histories.append(history)\n            model.test_iterator.imprs.append(impr_news)\n            model.test_iterator.labels.append(label)\n            model.test_iterator.impr_indexes.append(impr_index)\n            model.test_iterator.uindexes.append(uindex)\n            impr_index += 1\n\n# Apply patch\nmodel.test_iterator.init_behaviors = init_behaviors_no_labels\n\nprint(\"‚úÖ Iterator patched for test set\")\n\n# Run evaluationa\nprint(\"\\nüîç Running evaluation on test set...\")\nprint(\"   Model will use trained weights to generate rankings\")\n\n# API-compatible evaluation: try fast_eval first, fallback to eval\ntry:\n    # Check if run_fast_eval method exists and works\n    if hasattr(model, 'run_fast_eval') and hasattr(model, '_get_news_feature_from_iter'):\n        print(\"   Using run_fast_eval (fast method)...\")\n        group_impr_indexes, group_labels, group_preds = model.run_fast_eval(\n            test_news_file, test_behaviors_file\n        )\n    else:\n        print(\"   Using run_eval (compatible method)...\")\n        eval_results = model.run_eval(test_news_file, test_behaviors_file)\n        # Extract results from eval_results dict if needed\n        if isinstance(eval_results, dict):\n            group_impr_indexes = eval_results.get('group_impr_indexes', [])\n            group_labels = eval_results.get('group_labels', [])\n            group_preds = eval_results.get('group_preds', [])\n        else:\n            # If eval_results returns tuple directly\n            group_impr_indexes, group_labels, group_preds = eval_results\nexcept AttributeError as e:\n    print(f\"   Fast eval failed: {e}\")\n    print(\"   Falling back to run_eval...\")\n    eval_results = model.run_eval(test_news_file, test_behaviors_file)\n    if isinstance(eval_results, dict):\n        group_impr_indexes = eval_results.get('group_impr_indexes', [])\n        group_labels = eval_results.get('group_labels', [])\n        group_preds = eval_results.get('group_preds', [])\n    else:\n        group_impr_indexes, group_labels, group_preds = eval_results\n\nprint(f\"\\n‚úÖ Generated predictions for {len(group_impr_indexes)} impressions\")\n\n# Write predictions\nprint(\"\\nüíæ Writing predictions to file...\")\nprediction_file = \"/kaggle/working/prediction.txt\"\n\nwith open(prediction_file, 'w') as f:  \n    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds), \n                                   total=len(group_impr_indexes),\n                                   desc=\"Writing\"):  \n        # MIND competition format: impression_id b·∫Øt ƒë·∫ßu t·ª´ 1\n        impr_id = impr_index + 1\n        \n        # Calculate rankings (score cao nh·∫•t = rank 1)\n        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()  \n        pred_rank_str = '[' + ','.join([str(i) for i in pred_rank]) + ']'  \n        \n        f.write(f\"{impr_id} {pred_rank_str}\\n\")\n\nprint(f\"\\n‚úÖ Prediction file saved to: {prediction_file}\")\n\n# Validate output\nprint(\"\\nüîç Validating output format...\")\nwith open(prediction_file, 'r') as f:\n    lines = f.readlines()\n    print(f\"   Total predictions: {len(lines)}\")\n    print(f\"   First 3 lines:\")\n    for i in range(min(3, len(lines))):\n        parts = lines[i].strip().split()\n        print(f\"      ImprID={parts[0]}, Rankings={parts[1][:50]}{'...' if len(parts[1]) > 50 else ''}\")\n\nprint(\"\\n‚úÖ File ready to submit to MIND competition!\")\nprint(f\"   Download: {prediction_file}\")",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import zipfile\nimport os\n\n# ƒê∆∞·ªùng d·∫´n file input (ƒë√£ t·∫°o ·ªü b∆∞·ªõc tr∆∞·ªõc) v√† output\nsource_file = \"/kaggle/working/prediction.txt\"\nzip_output_path = \"/kaggle/working/prediction.zip\"\n\n\ntry:\n    # T·∫°o file zip v·ªõi ch·∫ø ƒë·ªô n√©n ZIP_DEFLATED\n    with zipfile.ZipFile(zip_output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # arcname='prediction.txt' ƒë·∫£m b·∫£o file trong zip ch·ªâ c√≥ t√™n l√† prediction.txt\n        # ch·ª© kh√¥ng ch·ª©a ƒë∆∞·ªùng d·∫´n th∆∞ m·ª•c m·∫π (/kaggle/working/...)\n        zipf.write(source_file, arcname='prediction.txt')\n\n\n    # (T√πy ch·ªçn) Ki·ªÉm tra n·ªôi dung b√™n trong file zip v·ª´a t·∫°o\n    with zipfile.ZipFile(zip_output_path, 'r') as zipf:\n        for info in zipf.infolist():\n            print(f\"File: {info.filename} | Size: {info.file_size / 1024:.2f} KB\")\n\nexcept Exception as e:\n    print(e)",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
